import time
from typing import Optional, Tuple, Union

import gym
import numpy as np

from stable_baselines3.common.type_aliases import GymStepReturn
from stable_baselines3.common.monitor import Monitor as _Monitor


class Monitor(_Monitor):
    def __init__(
        self,
        env: gym.Env,
        filename: Optional[str] = None,
        allow_early_resets: bool = True,
        reset_keywords: Tuple[str, ...] = (),
        info_keywords: Tuple[str, ...] = (),
        metric_fn=None,
        max_distance=1,
    ):
        super(Monitor, self).__init__(
            env=env,
            filename=filename,
            allow_early_resets=allow_early_resets,
            reset_keywords=reset_keywords,
            info_keywords=info_keywords,
        )
        self._metric_fn = metric_fn
        self._max_distance = max_distance

    def step(self, action: Union[np.ndarray, int]) -> GymStepReturn:
        """
        Step the environment with the given action

        :param action: the action
        :return: observation, reward, done, information
        """
        if self.needs_reset:
            raise RuntimeError("Tried to step environment that needs reset")
        observation, reward, done, info = self.env.step(action)
        self.rewards.append(reward)
        if done:
            self.needs_reset = True
            ep_rew = sum(self.rewards)
            ep_len = len(self.rewards)
            ep_info = {
                "r": round(ep_rew, 6),
                "l": ep_len,
                "t": round(time.time() - self.t_start, 6),
            }
            for key in self.info_keywords:
                ep_info[key] = info[key]
            self.episode_returns.append(ep_rew)
            self.episode_lengths.append(ep_len)
            self.episode_times.append(time.time() - self.t_start)
            ep_info.update(self.current_reset_info)
            raw_distance = self._metric_fn(
                observation["achieved_goal"], observation["desired_goal"]
            )
            distance_info = {
                "raw_distance": raw_distance,
                "normalized_distance_score": (self._max_distance - raw_distance)
                / self._max_distance,
            }
            ep_info.update(distance_info)
            if self.results_writer:
                self.results_writer.write_row(ep_info)
            info["episode"] = ep_info
        self.total_steps += 1
        return observation, reward, done, info
